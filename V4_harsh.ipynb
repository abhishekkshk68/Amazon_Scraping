{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V3_harsh.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOraGIDHlql2hxw+g6MVeuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekkshk68/Amazon_Scraping/blob/master/V4_harsh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ1aYCQSyY6o",
        "outputId": "7c23ad97-d76c-4745-e96e-627070cbe8f8"
      },
      "source": [
        " !pip install html2text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting html2text\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2020.1.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5NVud__yee1"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiRAdwDbyg7z",
        "outputId": "3f0c525d-a9a7-4f66-d80c-5b0c7780b4d2"
      },
      "source": [
        "link='https://schools.org.in/gujrat'\n",
        "r = requests.get(link)\n",
        "data=r.text\n",
        "soup = BeautifulSoup(data, 'html.parser')\n",
        "title=soup.b.string\n",
        "s=title.replace(\" \", \"\").lower()\n",
        "print(s)\n",
        "soup.findAll('my-3 p-3 bg-white rounded shadow-sm')\n",
        "city_name=[]\n",
        "#print(title)\n",
        "for x in soup.find_all('a'):  \n",
        "    #if title in x.get('href'):\n",
        "      m=str(x.get('href'))\n",
        "      #print(m)\n",
        "      if m.startswith(s)==True:\n",
        "        city_name.append(m)\n",
        "city_link=[]\n",
        "#with open (s+\".csv\",'a') as f:\n",
        "  #f. write(s +\"\\t\"+ \"links\"+\"\\n\") \n",
        "mainlink='https://schools.org.in/'\n",
        "state_file_name=s+\".csv\"\n",
        "for x in city_name:\n",
        "  complete_link=mainlink+x\n",
        "  city_link.append(complete_link)\n",
        "  with open (state_file_name,'a') as f:\n",
        "    f. write(complete_link+\"\\n\") \n",
        "with open (state_file_name,\"r+\") as f:  \n",
        "   city_data=f.readlines() \n",
        "   for x in city_data:  \n",
        "      #x='https://schools.org.in/gujrat/ahmedabad'\n",
        "      #print(x)\n",
        "      r1 = requests.get(x)\n",
        "      sub=x[23:-1]\n",
        "      startsub=x[0:23]\n",
        "      #print(startsub)\n",
        "      #print(sub)\n",
        "      data1=r1.text\n",
        "      soup1 = BeautifulSoup(data1, 'html.parser')\n",
        "      s1=str(soup1.b.string.replace(\"Schools in \",\"\")).lower()\n",
        "      s1.replace(\" \",\"\")\n",
        "      for x in soup1.find_all('a'):  \n",
        "          #if title in x.get('href'):\n",
        "            clsuter_name=\"cluster\"+\"_\"+s+\".csv\"\n",
        "            m=str(x.get('href')).replace(\" \",\"\")\n",
        "            with open (clsuter_name,\"a+\") as f:\n",
        "                if sub in m:\n",
        "                  clustrlink=startsub+m\n",
        "                  f.write(clustrlink+\"\\n\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gujrat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3s6iLflymRP"
      },
      "source": [
        "with open (clsuter_name) as cf:\n",
        "  cf_data=cf.readlines()\n",
        "  city_name_real=s+\"_city\"+\".csv\"\n",
        "  for xt in cf_data:\n",
        "    r2 = requests.get(xt)\n",
        "    data2=r2.text\n",
        "    soup1 = BeautifulSoup(data2, 'html.parser')\n",
        "    s2=str(soup1.b.string.replace(\"Schools in \",\"\")).lower()\n",
        "    sub1=xt[23:-1]\n",
        "    startsub1=xt[0:23]\n",
        "    #print(s2)\n",
        "    \n",
        "    \n",
        "    for x in soup1.find_all('a'):  \n",
        "      #if title in x.get('href'):\n",
        "          mc=str(x.get('href')).replace(\" \",\"\")\n",
        "          if sub1 in mc:\n",
        "              with open (city_name_real,\"a+\") as fcn:\n",
        "                city=startsub+mc\n",
        "                fcn.write(city+\"\\n\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbspl7-WyrQf"
      },
      "source": [
        "newfile=s+\"School_city\"+\".csv \"\n",
        "\n",
        "with open (city_name_real,\"r+\") as fsc: \n",
        "  data_fsc=fsc.readlines()\n",
        "  for xts in data_fsc:\n",
        "        #xts='https://schools.org.in/gujrat/ahmedabad/amc/amraiwadi'\n",
        "        #print(xts)\n",
        "        r4 = requests.get(xts)\n",
        "        data4=r4.text\n",
        "        soup4 = BeautifulSoup(data4, 'html.parser')\n",
        "        s3=str(soup4.b.string.replace(\"Schools in \",\"\")).lower()\n",
        "        #print(s3)\n",
        "\n",
        "        for x in soup4.find_all('a'):    \n",
        "\n",
        "                m=str(x.get('href')).replace(\" \",\"\")\n",
        "                if \".html\" in m:\n",
        "                  if \"https\" not in m:\n",
        "                    #print(len(s1))\n",
        "                    #print(m)\n",
        "                    k=s3.replace(\" \",\"\")\n",
        "                    #print(len(k))\n",
        "                    if k in m:\n",
        "                      school_link=mainlink+m\n",
        "                      with open (newfile,'a+') as fschool_name:  \n",
        "                          fschool_name.write(school_link+\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPWiddQUzkqs"
      },
      "source": [
        "#import re\n",
        "import html2text\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#newfile=\"/content/gujratSchool_city.csv \"\n",
        "#school_link=\"https://schools.org.in/ahmedabad/24071203265/shivam-english-school.html\"\n",
        "with open(newfile,\"r+\") as fschool:     \n",
        "      school_data_links=fschool.readlines()\n",
        "      for school_link in school_data_links:\n",
        "      #school_link=\"https://schools.org.in/ahmedabad/24070200101/abasana-primary-school.html\"\n",
        "              r3 = requests.get(school_link)\n",
        "              data3=r3.text\n",
        "              soup3 = BeautifulSoup(data3, 'html.parser')\n",
        "              title_school=soup3.title.string\n",
        "              print(title_school)\n",
        "              file_name=title_school[0:15]\n",
        "              #for x in soup3.find_all('b'):\n",
        "                #print(x)\n",
        "\n",
        "              #for x in soup3.find_all(\"div\", attrs={\"class\": \"list-group-item list-group-item-light\"}):\n",
        "              # print(x)\n",
        "\n",
        "              ms=soup3.find_all(\"div\", attrs={\"class\": \"col-lg-6\"})\n",
        "              # initializing tag\n",
        "              #print(ms)\n",
        "              ts=soup3.find_all(\"div\", attrs={\"class\": \"my-3 p-3 bg-white rounded shadow-sm\"})\n",
        "                \n",
        "              # regex to extract required strings\n",
        "              #reg_str = \"<.*?>\"\n",
        "              #headers = soup3.find_all(['li'])\n",
        "              \n",
        "              for x in ts:\n",
        "                text = html2text.html2text(str(x))\n",
        "                st=text.split(\"**\")\n",
        "                with open (file_name+\"less.tsv\", 'a') as f:\n",
        "                  for stt in st:\n",
        "                    t=stt.replace('  * __','')\n",
        "                    f.write(t + \"\\t\")\n",
        "                  f.write(\"\\n\")\n",
        "               \n",
        "          \n",
        "  #print(\"####################\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgBrYWrx7iMe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}